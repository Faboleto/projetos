{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5735c19c-8a76-49c9-95df-6c168721c8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import psycopg2\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, DoubleType, DecimalType, BooleanType, DateType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "url = dbutils.secrets.get(\"#####-secrets\", \"#####-url\")\n",
    "user = dbutils.secrets.get(\"#####-secrets\", \"#####-user\")\n",
    "password = dbutils.secrets.get(\"#####-secrets\", \"#####-password\")\n",
    "\n",
    "PORT = 5432\n",
    "DATABASE_NAME = '#####'\n",
    "\n",
    "INITIAL_START_DATE_STR = '2025-07-25T00:00:00.00'\n",
    "NUM_DAYS_TO_LOAD = 30\n",
    "DEFAULT_INCREMENT_INTERVAL_HOURS = 24\n",
    "\n",
    "def map_postgres_to_spark_type(pg_type_code, pg_type_name):\n",
    "    \"\"\"\n",
    "    Mapeia os tipos de dados do PostgreSQL para os tipos de dados do Spark.\n",
    "    \"\"\"\n",
    "    if pg_type_name in ('int2', 'int4'):\n",
    "        return IntegerType()\n",
    "    elif pg_type_name in ('int8', 'bigint'):\n",
    "        return LongType()\n",
    "    elif pg_type_name in ('float4', 'real'):\n",
    "        return FloatType()\n",
    "    elif pg_type_name in ('float8', 'double precision'):\n",
    "        return DoubleType()\n",
    "    elif pg_type_name in ('numeric', 'decimal'):\n",
    "        return DecimalType(38, 18)\n",
    "    elif pg_type_name in ('bool', 'boolean'):\n",
    "        return BooleanType()\n",
    "    elif pg_type_name in ('date'):\n",
    "        return DateType()\n",
    "    elif pg_type_name in ('timestamp', 'timestamptz'):\n",
    "        return TimestampType()\n",
    "    elif pg_type_name in ('jsonb', 'json', 'text', 'varchar', 'char', 'uuid', 'cidr', 'inet', 'macaddr'):\n",
    "        return StringType()\n",
    "    \n",
    "    print(f\"Aviso: Tipo PostgreSQL '{pg_type_name}' (OID: {pg_type_code}) não tem mapeamento direto para Spark. Usando StringType.\")\n",
    "    return StringType()\n",
    "\n",
    "def get_postgres_connection(db_url, db_user, db_password, db_port, db_name):\n",
    "    \"\"\"\n",
    "    Cria e retorna uma conexão com o banco de dados PostgreSQL.\n",
    "    \"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host=db_url,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        port=db_port,\n",
    "        dbname=db_name\n",
    "    )\n",
    "\n",
    "def infer_and_get_schema(conn, sql_query_template):\n",
    "    \"\"\"\n",
    "    Inferencia o schema PySpark a partir de uma query SQL de template,\n",
    "    consultando o banco de dados PostgreSQL.\n",
    "    \"\"\"\n",
    "    cur = None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        if \"WHERE\" in sql_query_template.upper():\n",
    "            base_query_for_schema = sql_query_template.split(\"WHERE\")[0].strip()\n",
    "        else:\n",
    "            base_query_for_schema = sql_query_template.strip()\n",
    "        \n",
    "        schema_query = f\"{base_query_for_schema} LIMIT 1;\"\n",
    "        \n",
    "        print(f\"Inferindo schema com query: {schema_query}\")\n",
    "        cur.execute(schema_query)\n",
    "\n",
    "        column_descriptions = cur.description\n",
    "        spark_schema_fields = []\n",
    "        for col_desc in column_descriptions:\n",
    "            col_name = col_desc[0]\n",
    "            pg_type_code = col_desc[1]\n",
    "\n",
    "            cur_type_name = conn.cursor()\n",
    "            cur_type_name.execute(f\"SELECT typname FROM pg_type WHERE oid = {pg_type_code};\")\n",
    "            pg_type_name = cur_type_name.fetchone()[0]\n",
    "            cur_type_name.close()\n",
    "\n",
    "            spark_type = map_postgres_to_spark_type(pg_type_code, pg_type_name)\n",
    "            spark_schema_fields.append(StructField(col_name, spark_type, True))\n",
    "\n",
    "        schema = StructType(spark_schema_fields)\n",
    "        return schema\n",
    "    finally:\n",
    "        if cur:\n",
    "            cur.close()\n",
    "\n",
    "def get_max_updated_at_from_unity_catalog(unity_catalog_table_name):\n",
    "    \"\"\"\n",
    "    Busca a data máxima da coluna 'updated_at' na tabela do Unity Catalog.\n",
    "    Retorna None se a tabela não existir ou estiver vazia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {unity_catalog_table_name}\").collect()\n",
    "        \n",
    "        max_date_df = spark.sql(f\"SELECT MAX(updated_at) FROM {unity_catalog_table_name}\")\n",
    "        max_date_row = max_date_df.collect()[0]\n",
    "        \n",
    "        if max_date_row[0] is not None:\n",
    "            print(f\"Data máxima 'updated_at' encontrada no Unity Catalog: {max_date_row[0]}\")\n",
    "            return max_date_row[0]\n",
    "        else:\n",
    "            print(f\"Tabela '{unity_catalog_table_name}' no Unity Catalog está vazia.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao buscar data máxima no Unity Catalog (tabela pode não existir): {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_data_to_unity_catalog(\n",
    "    catalog_name,\n",
    "    schema_name,\n",
    "    table_name,\n",
    "    base_sql_query_template,\n",
    "    initial_start_dt_str=None,\n",
    "    num_days_to_load=NUM_DAYS_TO_LOAD,\n",
    "    increment_interval_hours=DEFAULT_INCREMENT_INTERVAL_HOURS\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingere dados de forma incremental de uma base de dados PostgreSQL para uma tabela\n",
    "    no Unity Catalog do Databricks, com base na coluna 'updated_at'.\n",
    "    \n",
    "    Args:\n",
    "        catalog_name (str): O nome do catálogo no Unity Catalog.\n",
    "        schema_name (str): O nome do schema de destino.\n",
    "        table_name (str): O nome da tabela de destino.\n",
    "        base_sql_query_template (str): A query SQL base para buscar os dados do PostgreSQL.\n",
    "            Deve conter uma cláusula `WHERE` para ser complementada pela função.\n",
    "        initial_start_dt_str (str, opcional): Data de início para a carga inicial no formato\n",
    "            'YYYY-MM-DDTHH:MM:SS.ff'. Se None, usará o valor padrão da constante.\n",
    "            Padrão: None.\n",
    "        num_days_to_load (int): Número de dias a serem carregados a partir da data de início.\n",
    "            Padrão: 30.\n",
    "        increment_interval_hours (int): O intervalo de tempo, em horas, para cada carga incremental.\n",
    "            Padrão: 24 horas.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_date_to_use = initial_start_dt_str if initial_start_dt_str is not None else INITIAL_START_DATE_STR\n",
    "\n",
    "    unity_catalog_table = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    interval_td = timedelta(hours=increment_interval_hours)\n",
    "\n",
    "    max_uc_date = get_max_updated_at_from_unity_catalog(unity_catalog_table)\n",
    "    \n",
    "    if max_uc_date:\n",
    "        current_interval_start = max_uc_date + timedelta(microseconds=1)\n",
    "        print(f\"Iniciando carga incremental a partir da data máxima do Unity Catalog: {current_interval_start}\")\n",
    "    else:\n",
    "        current_interval_start = datetime.strptime(start_date_to_use, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "        print(f\"Iniciando carga inicial a partir de: {current_interval_start}\")\n",
    "\n",
    "    calculated_end_datetime = current_interval_start + timedelta(days=num_days_to_load)\n",
    "    end_datetime = min(calculated_end_datetime, datetime.now())\n",
    "    \n",
    "    if current_interval_start >= end_datetime:\n",
    "        print(f\"A data de início ({current_interval_start}) é igual ou posterior à data de fim calculada ({end_datetime}). Nenhuma nova carga será realizada.\")\n",
    "        return\n",
    "\n",
    "    conn_schema = None\n",
    "    inferred_schema = None\n",
    "    try:\n",
    "        conn_schema = get_postgres_connection(url, user, password, PORT, DATABASE_NAME)\n",
    "        inferred_schema = infer_and_get_schema(conn_schema, base_sql_query_template)\n",
    "        print(\"--- Schema PySpark Inferido ---\")\n",
    "        spark.createDataFrame([], inferred_schema).printSchema()\n",
    "        print(\"-------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inferir o schema: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        if conn_schema:\n",
    "            conn_schema.close()\n",
    "\n",
    "    if not inferred_schema:\n",
    "        print(\"Não foi possível inferir o schema. Abortando.\")\n",
    "        return\n",
    "\n",
    "    while current_interval_start < end_datetime:\n",
    "        current_interval_end = current_interval_start + interval_td\n",
    "\n",
    "        if current_interval_end > end_datetime:\n",
    "            current_interval_end = end_datetime\n",
    "\n",
    "        start_date_str_sql = current_interval_start.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        end_date_str_sql = current_interval_end.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "        sql_query = f\"{base_sql_query_template.strip()} AND updated_at >= '{start_date_str_sql}' AND updated_at < '{end_date_str_sql}'\"\n",
    "\n",
    "        print(f\"\\n--- Carregando dados para o período: {start_date_str_sql} a {end_date_str_sql} ---\")\n",
    "        print(f\"Query SQL: {sql_query}\")\n",
    "\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_postgres_connection(url, user, password, PORT, DATABASE_NAME)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(sql_query)\n",
    "            rows = cur.fetchall()\n",
    "            cur.close()\n",
    "\n",
    "            if rows:\n",
    "                df = spark.createDataFrame(rows, schema=inferred_schema)\n",
    "                print(f\"DataFrame PySpark criado com {df.count()} linhas.\")\n",
    "                \n",
    "                df.write.mode(\"append\").saveAsTable(unity_catalog_table)\n",
    "                print(f\"Dados gravados com sucesso na tabela do Unity Catalog: {unity_catalog_table}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"Nenhum dado encontrado para este período.\")\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Erro ao conectar, consultar ou gravar o banco de dados para o período {start_date_str_sql} a {end_date_str_sql}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro inesperado durante o carregamento do período {current_interval_start} a {end_date_str_sql}: {e}\")\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "        \n",
    "        current_interval_start = current_interval_end\n",
    "        \n",
    "        if current_interval_start >= end_datetime:\n",
    "             break\n",
    "\n",
    "    print(\"\\n--- Carregamento incremental concluído ---\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "postgres_to_unity_catalog_library.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
